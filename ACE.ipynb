{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.commons import *\n",
    "import torch\n",
    "from torchattacks import CW\n",
    "from torch import optim\n",
    "import math\n",
    "import csv\n",
    "# torch.cuda.set_device(3)\n",
    "from torchvision.models import EfficientNet_B0_Weights, efficientnet_b0\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CE(CW):\n",
    "    \n",
    "    def __init__(self, model, c=1, kappa=0, steps=50, lr=0.01, lamb=1):\n",
    "        super(CE, self).__init__(model, c, kappa, steps, lr)\n",
    "        self.c = c\n",
    "        self.kappa = kappa\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        self.lamb = lamb\n",
    "        self.supported_mode = ['default', 'targeted']\n",
    "\n",
    "    def attack_uncertainty(self, loader, return_type=\"loader\"):\n",
    "        ces = []\n",
    "        ys = []\n",
    "        for i ,(X, y) in enumerate(loader):\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"Processing {i+1}/{len(loader)}\")\n",
    "            ce_X = self.__call__(X, y).clone().detach()\n",
    "            ces.append(ce_X)\n",
    "            ys.append(y)\n",
    "        if return_type == \"loader\":\n",
    "            return wrapper.to_loader((torch.cat(ces), torch.cat(ys)), batch_size = 1)\n",
    "        elif return_type == \"tensor\":\n",
    "            return torch.cat(ces)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def h_loss(self, images, labels):\n",
    "        logits = self.get_logits(images)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        conf, pred_labels = torch.max(probs, dim=1)\n",
    "        \n",
    "        indices = labels.view(-1, 1).to(torch.int64)\n",
    "        fx_y = torch.gather(probs, 1, indices).squeeze()\n",
    "\n",
    "        return fx_y\n",
    "\n",
    "    \n",
    "    def h_loss_2(self, images, init_labels, sign):\n",
    "        logits = self.get_logits(images)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        indices = init_labels.view(-1, 1).to(torch.int64)\n",
    "        fx_y = torch.gather(probs, 1, indices).squeeze()\n",
    "        return sign * fx_y\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "                \n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        init_logits = self.get_logits(images)\n",
    "        init_probs = torch.softmax(init_logits, dim=-1)\n",
    "        init_confs, init_pred_labels = torch.max(init_probs, dim=-1)\n",
    "        \n",
    "        sign = 2 * (init_pred_labels == labels) - 1\n",
    "\n",
    "        w = self.inverse_tanh_space(images).detach()\n",
    "        w.requires_grad = True\n",
    "\n",
    "        best_adv_images = images.clone().detach()\n",
    "        best_L2 = 1e10*torch.ones((len(images))).to(self.device)\n",
    "        prev_cost = 1e10\n",
    "        dim = len(images.shape)\n",
    "\n",
    "        MSELoss = nn.MSELoss(reduction='none')\n",
    "        Flatten = nn.Flatten()\n",
    "\n",
    "        optimizer = optim.Adam([w], lr=self.lr)\n",
    "\n",
    "        for step in range(self.steps):\n",
    "            # Get adversarial images\n",
    "            adv_images = self.tanh_space(w)\n",
    "\n",
    "            # Calculate loss\n",
    "            current_L2 = MSELoss(Flatten(adv_images),\n",
    "                                 Flatten(images)).sum(dim=1)\n",
    "            L2_loss = current_L2.sum()\n",
    "\n",
    "            outputs = self.get_logits(adv_images)\n",
    "            \n",
    "            # h_loss = self.h_loss(adv_images, labels)\n",
    "            \n",
    "            h_loss = self.h_loss_2(adv_images, init_pred_labels, sign)\n",
    "            # print(L2_loss,\"           \", h_loss)\n",
    "            cost = L2_loss + self.c * h_loss.sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update adversarial images\n",
    "            _, pre = torch.max(outputs.detach(), 1)\n",
    "            correct = (pre == labels).float()\n",
    "\n",
    "            # mask = torch.zeros_like(correct)\n",
    "            logits = self.get_logits(adv_images)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            pred_labels = torch.argmax(probs, dim=-1)\n",
    "            confs, _ = torch.max(probs, dim=1)\n",
    "            \n",
    "            mask = 1 * (\n",
    "                (~ torch.logical_xor(init_pred_labels == labels, (init_confs - confs) > 0))\n",
    "                & (pred_labels == init_pred_labels)\n",
    "            )\n",
    "            \n",
    "           \n",
    "            mask = mask.view([-1]+[1]*(dim-1))\n",
    "            best_adv_images = mask*adv_images.detach() + (1-mask)*best_adv_images \n",
    "\n",
    "        return best_adv_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCA_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CE_Black(CW):\n",
    "    \n",
    "    def __init__(self, model, model2, c=1, kappa=0, steps=50, lr=0.01, lamb=1):\n",
    "        super(CE_Black, self).__init__(model, c, kappa, steps, lr)\n",
    "        self.c = c\n",
    "        self.kappa = kappa\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        self.lamb = lamb\n",
    "        self.supported_mode = ['default', 'targeted']\n",
    "        self.model2 = model2\n",
    "\n",
    "    def attack_uncertainty(self, loader, return_type=\"loader\"):\n",
    "        ces = []\n",
    "        ys = []\n",
    "        for i ,(X, y) in enumerate(loader):\n",
    "            # print(i)\n",
    "            ce_X = self.__call__(X, y, self.model2).clone().detach()\n",
    "            # print(ce_X.shape, y.shape)\n",
    "            ces.append(ce_X)\n",
    "            ys.append(y)\n",
    "        if return_type == \"loader\":\n",
    "            return wrapper.to_loader((torch.cat(ces), torch.cat(ys)), batch_size=8)\n",
    "            # return ces, ys\n",
    "        elif return_type == \"tensor\":\n",
    "            return torch.cat(ces)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def h_loss(self, images, labels):\n",
    "        logits = self.get_logits(images)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        conf, pred_labels = torch.max(probs, dim=1)\n",
    "        \n",
    "        indices = labels.view(-1, 1).to(torch.int64)\n",
    "        fx_y = torch.gather(probs, 1, indices).squeeze()\n",
    "\n",
    "        return fx_y\n",
    "        # return 1 / fx_y * torch.log10(conf - fx_y + 1 + 1e-6) + self.lamb * fx_y\n",
    "    \n",
    "    def h_loss_2(self, images, init_labels, sign):\n",
    "        logits = self.get_logits(images)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        indices = init_labels.view(-1, 1).to(torch.int64)\n",
    "        fx_y = torch.gather(probs, 1, indices).squeeze()\n",
    "        return sign * fx_y\n",
    "\n",
    "    def forward(self, images, labels, model2):\n",
    "                \n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        # init_logits = self.get_logits(images)\n",
    "        inputs = images\n",
    "        init_logits = model2(inputs)\n",
    "        init_probs = torch.softmax(init_logits, dim=-1)\n",
    "        init_confs, init_pred_labels = torch.max(init_probs, dim=-1)\n",
    "        \n",
    "        sign = 2 * (init_pred_labels == labels) - 1\n",
    "\n",
    "        # w = torch.zeros_like(images).detach() # Requires 2x times\n",
    "        w = self.inverse_tanh_space(images).detach()\n",
    "        w.requires_grad = True\n",
    "\n",
    "        best_adv_images = images.clone().detach()\n",
    "        best_L2 = 1e10*torch.ones((len(images))).to(self.device)\n",
    "        prev_cost = 1e10\n",
    "        dim = len(images.shape)\n",
    "\n",
    "        MSELoss = nn.MSELoss(reduction='none')\n",
    "        Flatten = nn.Flatten()\n",
    "\n",
    "        optimizer = optim.Adam([w], lr=self.lr)\n",
    "\n",
    "        for step in range(self.steps):\n",
    "            # Get adversarial images\n",
    "            adv_images = self.tanh_space(w)\n",
    "\n",
    "            # Calculate loss\n",
    "            current_L2 = MSELoss(Flatten(adv_images),\n",
    "                                 Flatten(images)).sum(dim=1)\n",
    "            L2_loss = current_L2.sum()\n",
    "\n",
    "            outputs = self.get_logits(adv_images)\n",
    "            \n",
    "            # h_loss = self.h_loss(adv_images, labels)\n",
    "            \n",
    "            h_loss = self.h_loss_2(adv_images, init_pred_labels, sign)\n",
    "            # print(L2_loss,\"           \", h_loss)\n",
    "            cost = L2_loss + self.c * h_loss.sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update adversarial images\n",
    "            _, pre = torch.max(outputs.detach(), 1)\n",
    "            correct = (pre == labels).float()\n",
    "\n",
    "            # mask = torch.zeros_like(correct)\n",
    "            logits = self.get_logits(adv_images)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            pred_labels = torch.argmax(probs, dim=-1)\n",
    "            confs, _ = torch.max(probs, dim=1)\n",
    "            \n",
    "            \n",
    "            logits_2 = model2(adv_images)\n",
    "            probs_2 = torch.softmax(logits_2, dim=-1)\n",
    "            pred_labels_2 = torch.argmax(probs_2, dim=-1)\n",
    "            confs_2, _ = torch.max(probs_2, dim=1)\n",
    "            \n",
    "            # mask = 1 * ((init_confs - confs > 0) & (pred_labels == init_pred_labels))\n",
    "            \n",
    "            mask = 1 * (\n",
    "                (~ torch.logical_xor(init_pred_labels == labels, (init_confs - confs_2) > 0))\n",
    "                & (pred_labels_2 == init_pred_labels)\n",
    "            )\n",
    "            \n",
    "            # print(init_confs)\n",
    "            # print(confs)\n",
    "            # print(\"comp: \", (init_confs - confs > 0))\n",
    "                  \n",
    "            # print(pred_labels)\n",
    "            # print(labels)\n",
    "            # print(\"comp: \", pred_labels == labels)\n",
    "            # print((init_confs - confs > 0) & (pred_labels == labels))\n",
    "            \n",
    "            # print(mask)\n",
    "            mask = mask.view([-1]+[1]*(dim-1))\n",
    "            best_adv_images = mask*adv_images.detach() + (1-mask)*best_adv_images\n",
    "            \n",
    "            # with torch.no_grad():\n",
    "            #     best_logits = self.get_logits(best_adv_images)\n",
    "            #     best_probs = torch.softmax(best_logits, dim=-1)\n",
    "            #     best_confs, best_labels = torch.max(best_probs, dim=-1)\n",
    "                \n",
    "            # satisfied_index = ~ torch.logical_xor(init_pred_labels == best_labels, (init_confs - best_confs) > 0)\n",
    "            # satisfied_count = torch.count_nonzero(satisfied_index)\n",
    "            # if satisfied_count / best_labels.numel():\n",
    "            #     return best_adv_images\n",
    "            \n",
    "\n",
    "        return best_adv_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_response(logits):\n",
    "    return torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "def attack_confidence_estimation(model, input, label, normalization, proxy=None, epsilon=0.05, epsilon_decay=0.5, max_iterations=15, confidence_score_function=softmax_response, device='cuda'):\n",
    "    input = input.to(device)\n",
    "    label = label.to(device)\n",
    "    model = model.to(device)\n",
    "    data = normalization(input)\n",
    "    data.requires_grad = True\n",
    "    if proxy:\n",
    "        # Black-box setting, use proxy to calculate the gradients\n",
    "        proxy = proxy.to(device)\n",
    "        output = proxy(data)\n",
    "        proxy.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            model_output = model(normalization(input))\n",
    "    else:\n",
    "        # White-box setting, use model itself to calculate the gradients\n",
    "        output = model(data)\n",
    "        model.zero_grad()\n",
    "        model_output = output\n",
    "    init_prediction = model_output.argmax()\n",
    "    output = confidence_score_function(output)\n",
    "    # Calculate gradients of model in backward pass\n",
    "    output[0][init_prediction.item()].backward(retain_graph=True)\n",
    "    # Collect gradients\n",
    "    jacobian = data.grad.data\n",
    "    if init_prediction == label:\n",
    "        # If the model is correct, we wish to make it less confident of its prediction\n",
    "        attack_direction = -1\n",
    "    else:\n",
    "        # Otherwise, we wish to make it more confident of its misprediction\n",
    "        attack_direction = 1\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_iterations):\n",
    "            jacobian_sign = jacobian.sign()\n",
    "            perturbed_image = input + epsilon * jacobian_sign * attack_direction\n",
    "            perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "            new_output = model(normalization(perturbed_image))\n",
    "            if new_output.argmax() == init_prediction:\n",
    "                # This adversarial example does not change the prediction as required, return it\n",
    "                return perturbed_image\n",
    "            else:\n",
    "                epsilon = epsilon * epsilon_decay\n",
    "        # The attack has failed; either the epsilon was too large, epsilon_decay too small,\n",
    "        # or max_iterations was insufficient. Return original input.\n",
    "        return input\n",
    "\n",
    "def identity_transform(x):\n",
    "    return x\n",
    "\n",
    "def ACE(model, loader, device, epsilon=0.05, epsilon_decay=0.5, max_iterations=15, confidence_score_function=softmax_response, normalization=identity_transform):\n",
    "    adversarial_samples = []\n",
    "    labely = []\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        adversarial_sample = []\n",
    "        for i in range(images.size(0)):\n",
    "            input = images[i].unsqueeze(0)\n",
    "            true_label = labels[i]\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                original_output = model((input))\n",
    "            orig_prediction = torch.nn.functional.softmax(original_output, dim=1).max(1)\n",
    "\n",
    "            adversarial_example = attack_confidence_estimation(model=model, input=input, label=true_label, normalization=normalization)\n",
    "            with torch.no_grad():\n",
    "                attacked_output = model((adversarial_example))\n",
    "            attacked_prediction = torch.nn.functional.softmax(attacked_output, dim=1).max(1)\n",
    "            adversarial_example_np = adversarial_example.squeeze().detach().cpu().numpy()\n",
    "            adversarial_sample.append(adversarial_example.detach())\n",
    "        adversarial_sample = torch.cat(adversarial_sample, dim=0)\n",
    "        adversarial_samples.append(adversarial_sample)\n",
    "        labely.append(labels)\n",
    "    ACE_loader = wrapper.to_loader((torch.cat(adversarial_samples), torch.cat(labely)), batch_size=1)\n",
    "    return ACE_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from method import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "from torchvision.models import efficientnet_v2_l, EfficientNet_V2_L_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> Get Clean Data: CIFAR10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if (setting_name == \"ImageNet\"):\n",
    "    efficient = efficientnet_b0(EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    densenet = densenet121(DenseNet121_Weights.IMAGENET1K_V1)    \n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    resnet = resnet50(weights=weights)\n",
    "    dataset = get_dataset()\n",
    "else:\n",
    "    dataset = get_dataset()\n",
    "    model = get_network()\n",
    "    black_model = get_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = CE(model, lamb=10, steps=50, lr=0.1, c=1)\n",
    "if (setting_name == \"ImageNet\"):\n",
    "    ce.set_normalization_used(dataset.normalize.mean, dataset.normalize.std)\n",
    "else:\n",
    "    ce.set_normalization_used(* MEAN_STDs[setting_name])\n",
    "ce.set_device(\"cuda:3\")\n",
    "CE_loader = ce.attack_uncertainty(dataset.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACE_loader = ACE(model, dataset.test_loader, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_black = CE_Black(black_model, model, lamb=10, steps=50, lr=0.1, c=0.4)\n",
    "if (setting_name == \"ImageNet\"):\n",
    "    ce_black.set_normalization_used(dataset.normalize.mean, dataset.normalize.std)\n",
    "else:\n",
    "    ce_black.set_normalization_used(* MEAN_STDs[setting_name])\n",
    "ce_black.set_device(\"cuda\")\n",
    "CE_black_loader_kd1 = ce_black.attack_uncertainty(dataset.test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
